= Core Slick
Working with Relational Databases in Scala with Slick 2
:toc:
:source-highlighter: pygments

[[preface]]
== Preface

Slick is a Scala library for working with databases: querying, inserting data, updating data, and representing a schema.  Queries are written in Scala and type checked by the compiler. Slick aims to make working with a database similar to working with regular Scala collections.

This material is aimed at a Scala developer who has:

* taken the _Core Scala_ course or equivalent; 
* a good understanding of relation databases (rows, columns, joins, indexes) and SQL; and
* access to a relational database (PostgreSQL is the example we use).

It is useful to have experience using the SBT build tool.



////





////


== Orientation

(big picture example stuff)

(don't worry about details like projects and tags, we'll get to them later)

(followed by hands on getting there)


== Starting out with Slick

This section gets us working with Scala and Slick, creating a tables in a database, inserting rows, running simple queries.

=== Database Configuration

For this example we will use PostgreSQL 9, and a database called "core-slick":

[source,sql]
.Create a Database and Login from `psql`
----
CREATE DATABASE "core-slick" WITH ENCODING 'UTF8';
CREATE USER "core" WITH PASSWORD 'trustno1';
GRANT ALL ON DATABASE "core-slick" TO core;
----

Check you can login to this database:

[source,bash]
----
$ psql -d core-slick core
----


[CAUTION]
.Supported Databases
====
Slick supports PostgreSQL, MySQL, Derby, H2, SQLite, and Microsoft Access.

To work with DB2, SQL Server or Oracle you need a commercial license. These are the closed source _Slick Drivers_ known as the _Slick Extensions_. 

====


=== An SBT Project

To use Slick we create a regular Scala project and reference the Slick dependencies:

[source, scala]
.build.sbt
----
name := "core-slick-example"

version := "1.0"

scalaVersion := "2.10.3"

libraryDependencies += "com.typesafe.slick" %% "slick" % "2.0.1" 

libraryDependencies += "ch.qos.logback" % "logback-classic" % "1.1.2"

libraryDependencies += "org.postgresql" % "postgresql" % "9.3-1101-jdbc41"
----

(To do: explain the dependencies)


When you've created _build.sbt_ (or downloaded the example project), run SBT and the dependencies will be fetched.

[NOTE]
.IDEs
====
If you're working with IntelliJ IDEA or the Eclipse Scala IDE, our _core-slick-example_ project includes the plugins to generate the IDE project files:

----
sbt> eclipse
----

or

----
sbt> gen-idea
----

...and then open the project directory in your IDE.  For example, this is _File -> Import -> Existing Project_ menu.

====


=== Our First Table


[source,scala]
.schema1.scala
----
package underscoreio.schema

import scala.slick.driver.PostgresDriver.simple._

object Example1 extends App {

  class Planet(tag: Tag) extends Table[(Int,String,Double)](tag, "planet") {
    def id = column[Int]("id", O.PrimaryKey, O.AutoInc)
    def name = column[String]("name")
    def distance = column[Double]("distance_au")
    def * = (id, name, distance)
  }

  lazy val planets = TableQuery[Planet]

  Database.forURL("jdbc:postgresql:core-slick", user="core", password="trustno1", driver = "org.postgresql.Driver") withSession {
    implicit session =>
      planets.ddl.create
  }

}
----

Running this application will create the schema. You can run it from your IDE, or with `sbt run underscoreio.schema.Example1`.

If you example the schema, there should be no surprises here:

[source]
----
core-slick=# \d
             List of relations
 Schema |     Name      |   Type   | Owner
--------+---------------+----------+-------
 public | planet        | table    | core
 public | planet_id_seq | sequence | core
(2 rows)

core-slick=# \d planet
                                   Table "public.planet"
   Column    |          Type          |                      Modifiers
-------------+------------------------+-----------------------------------------------------
 id          | integer                | not null default nextval('planet_id_seq'::regclass)
 name        | character varying(254) | not null
 distance_au | double precision       | not null
Indexes:
    "planet_pkey" PRIMARY KEY, btree (id)
----



(lots to discuss about the code)

* What is a `Tag`?  "The Tag carries the information about the identity of the Table instance and how to create a new one with a different identity. Its implementation is hidden away in TableQuery.apply to prevent instantiation of Table objects outside of a TableQuery"

* Hoes does `Table[(Int,String)]` match up to `id` and `name` fields? - that's how Slick is going to represent rows. We can customize that to be something other than a tuple, a case class in particular.

* What is a projection (`*`) and why do I need to define it?  It's the default for queries and inserts. We will see how to convert this into more useful representation.

* What is a `TableQuery`?

* What is a session?

Note that driver is specified. You might want to mix in something else (e.g., H2 for testing).

Note we can talk about having longer column values later.

The `O` for PK or Auto means "Options".


==== Schema Creation

Our table, `planet`, was created with `table.dd.create`.  That's convenient for us, but Slick's schema management is very simple. For example, if you run `create` twice, you'll see:

----
org.postgresql.util.PSQLException: ERROR: relation "planet" already exists
----

That's because `create` blindly issues SQL commands:

[source,scala]
----
println(planets.ddl.createStatements.mkString)
----

...will output:

[source,sql]
----
create table "planet" ("id" SERIAL NOT NULL PRIMARY KEY,"name" VARCHAR(254) NOT NULL)
----

(There's a corresponding `dropStatements` that does the reverse).

To make our example easier to work with, we could query the database meta data and find out if our table already exists before we create it:

[source,scala]
----
if (MTable.getTables(planets.baseTableRow.tableName).firstOption.isEmpty)
  planets.ddl.create
----

However, for our simple example we'll end up dropping and creating the schema each time:

[source,scala]
----
MTable.getTables(planets.baseTableRow.tableName).firstOption match {
  case None =>
    planets.ddl.create
  case Some(t) =>
    planets.ddl.drop
    planets.ddl.create
 }
----

We'll look at other tools for managing schema migrations later.



=== Inserting Data


[source,scala]
----
// Populate with some data:

planets +=
  (100, "Earth",    1.0)

planets ++= Seq(
  (200, "Mercury",  0.4),
  (300, "Venus",    0.7),
  (400, "Mars" ,    1.5),
  (500, "Jupiter",  5.2),
  (600, "Saturn",   9.5),
  (700, "Uranus",  19.0),
  (800, "Neptune", 30.0)
)
----

Each `+=` or `++=` executes in its own transaction.

We've had to specify the id, name and distance, but this may be surprising because the ID is an auto incrementing field.  What Slick does, when inserting this data, is ignore the ID:

----
core-slick=# select * from planet;
 id |  name   | distance_au
----+---------+-------------
  1 | Earth   |           1
  2 | Mercury |         0.4
  3 | Venus   |         0.7
  4 | Mars    |         1.5
  5 | Jupiter |         5.2
  6 | Saturn  |         9.5
  7 | Uranus  |          19
  8 | Neptune |          30
(8 rows)
----

This is, generally, what you want to happen, and applies only to auto incrementing fields. If the ID was not auto incrementing, the ID values we supplied (100,200 and so on) would have been used.


NB: insert / forceInsert to ignore/include the ID column



=== A Simple Query

Let's fetch all the planets in the inner solar system:

[source,scala]
----
val query = for {
  planet <- planets
  if planet.distance < 5.0
} yield planet.name

println("Inner planets: " + query.run)
----

This produces:

----
Inner planets: Vector(Earth, Mercury, Venus, Mars)
----

What did Slick do to produce those results?  It ran this:

[source,sql]
----
select s9."name" from "planet" s9 where s9."distance_au" < 5.0
----

Note that it did not fetch all the planets and filter them. There's something more interesting going on that that.

.Logging What Slick is Doing
[NOTE]
===============================
Slick uses a logging framework called SLFJ.  You can configure this to capture information about the queries being run, and the log to different back ends.  The "core-slick-example" project uses a logging back-end called _Logback_, which is configured in the file _src/main/resources/logback.xml_.  In that file we enable statement logging by turning up the logging to debug level:

[source,xml]
----
<logger name="scala.slick.jdbc.JdbcBackend.statement" level="DEBUG"/>
----

When we next run a query, each statement will be recorded on standard output:

----
18:49:43.557 DEBUG s.slick.jdbc.JdbcBackend.statement - Preparing statement: drop table "planet"
18:49:43.564 DEBUG s.slick.jdbc.JdbcBackend.statement - Preparing statement: create table "planet" ("id" SERIAL NOT NULL PRIMARY KEY,"name" VARCHAR(254) NOT NULL,"distance_au" DOUBLE PRECISION NOT NULL)
----


You can enable a variety of events to be logged:

* `scala.slick.jdbc.JdbcBackend.statement` - which is for statement logging, as you've seen.
* `scala.slick.session` - for session information, such as connections being opened.
* `scala.slick` - for everything!  This is usually too much.

===============================




=== Running Queries in the REPL

----
> console
[info] Starting scala interpreter...
[info]
Welcome to Scala version 2.10.3 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_45).
Type in expressions to have them evaluated.
Type :help for more information.

scala> import scala.slick.driver.PostgresDriver.simple._
import scala.slick.driver.PostgresDriver.simple._

scala> import  underscoreio.schema.Example2._
import underscoreio.schema.Example2._

scala> implicit val session = Database.forURL("jdbc:postgresql:core-slick", user="core", password="trustno1", driver = "org.postgresql.Driver").createSession
session: slick.driver.PostgresDriver.backend.Session = scala.slick.jdbc.JdbcBackend$BaseSession@4f1e8443

planets.run
res0: Seq[(Int, String, Double)] = Vector((1,Earth,1.0), (2,Mercury,0.4), (3,Venus,0.7), (4,Mars,1.5), (5,Jupiter,5.2), (6,Saturn,9.5), (7,Uranus,19.0), (8,Neptune,30.0))

----

=== Exercises

What happens if you used 5 rather than 5.0 in the query?




=== Sorting


=== The Types Involved in a Query


=== Update





////




////



== Connecting, Transactions, Sessions


== Structuring the Schema

[source,scala]
----
object Tables extends {
   val profile = scala.slick.driver.PostgresDriver
} with Tables
----

(initialization pattern)


[source,scala]
----
trait Tables {
	val profile: scala.slick.drivers.JdbcProfile
	import profile.simple._

	// classes, implicits go here as usual.
}
----



== Table, Rows and Column Customisation

- NULL columms
- PK




== Joins

FK

t1.join(t2).on(condition)



== Queries Compose 

Reuse. 


Only runs when you say.  Keep to a `Query` for as long as possible.

== Query Extensions

E.g., pagination or byName("Mars")


== Dynamic Queries

need to upper case everything??

implict.... dynamicSort(keys: String*) : Query[T,E] = {
	keys match {
	  case nil = query
	  case h :: t => 
	  	dynamicSortImpl(t).sortBy( table => )
	  	// split h on . to get asc desc
	  h match {
	  	case name :: Nil =>  table.column[String](name).asc
	  	case _ => ???

	}
}
}

danger... access to user suppliued input!!

dynamicSort("street.desc", "city.desc")



== Working with Case Classes

----
case class Planet(val name: String, val size: Int)

class PlanetTable(tag: Tag) extends Table[Planet](tag, "planet") {
  def name = column[String]("name")
  def * = (name, distance).<>[Planet,Tuple2[String,Float]] (
    (t: Tuple2[String,Float]) => Planet.apply(t._1, t._2),
    (p: Planet) => Some(p.name, p.distance)
  )
}
----

----
  def * = (name, distance).<>[Planet.tupled, Planet.unapply] (
----


== Aggregations

counts, grouping and all that.


== Virtual Columns and Server Side Casts

def x = whatever

`asColumnOf[Double]`



== More on Joins

=== Outer Joins

`leftJoin` - dealing with NULL values

map all columns to option types via `.?` (nullable column)

slick will do this for you one day.


=== Auto Join

https://skillsmatter.com/skillscasts/4577-patterns-for-slick-database-applications

15:23 in

table1.joinOn(table2) : Query[(T1,T2),(Ta,Tb)]

via implicit joinCondition for T1,T2



== Custom Types

----
class SupplierId(val value: Int) extends AnyVal
 
case class Supplier(id: SupplierId, name: String, 
 city: String) 
 
implicit val supplierIdType = MappedColumnType.base 
 [SupplierId, Int](_.value, new SupplierId(_)) 
 
class Suppliers(tag: Tag) extends 
 Table[Supplier](tag, "SUPPLIERS") { 
 def id = column[SupplierId]("SUP_ID", ...) 
 ... 
} 
----


----

class SupplierId(val value: Int) extends MappedTo[Int] 
 
case class Supplier(id: SupplierId, name: String, 
 city: String) 

class Suppliers(tag: Tag) extends 
 Table[Supplier](tag, "SUPPLIERS") { 
 def id = column[SupplierId]("SUP_ID", ...) 
 ... 
}
----




== Dates and Time

Joda! See https://mackler.org/LearningSlick2/



== Testing



== Terminology

Lifted Embedding


== Plain SQL


== Code Generation for Existing Databases

- basic usage

- customizing (snake case v. camel case StringExtensions)

- SourceCodeGenerator(model).code hook for adding more stuff.
e.g., super.code + MORE STUFF. Nice autojoin example in https://skillsmatter.com/skillscasts/4577-patterns-for-slick-database-applications 31 mins in.



== Connection Pools


== Schema Migration


== Working with Multiple Databases




== Writing your own Driver




== Useful Links

* link to that nice mac postgresql app

* http://groups.google.com/group/scalaquery[The Slick Mailing List] (the group is called "scalaquery" as that was the original name for the technology that we now call Slick).

* http://www.postgresql.org/docs/9.3/static/index.html[PostgreSQL manual].



